{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class ConvExtractor(nn.Module):\n",
    "    def __init__(self, inplanes, planes=[128, 256, 512, 1024], stride=1, groups=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(64, planes[0], kernel_size=3, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = conv3x3(planes[0], planes[1], 2)\n",
    "        self.conv3 = conv3x3(planes[1], planes[2], 2)\n",
    "        #self.conv4 = conv3x3(inplanes, planes, 2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return_dict = {}\n",
    "        out = self.maxpool(self.relu(self.conv1(x)))\n",
    "        return_dict[\"layer1\"]\n",
    "    \n",
    "\n",
    "torch.rand((1,64,200,200))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import torch.nn.functional as F \n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torchvision.ops.boxes import box_area\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# modified from torchvision to also return the union\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dice_coef(inputs, targets):\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1).unsqueeze(1)\n",
    "    targets = targets.flatten(1).unsqueeze(0)\n",
    "    numerator = 2 * (inputs * targets).sum(2)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "\n",
    "    # NOTE coef doesn't be subtracted to 1 as it is not necessary for computing costs\n",
    "    coef = (numerator + 1) / (denominator + 1)\n",
    "    return coef\n",
    "\n",
    "\n",
    "def dice_loss(inputs, targets, num_boxes):\n",
    "    \"\"\"\n",
    "    Compute the DICE loss, similar to generalized IOU for masks\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "    \"\"\"\n",
    "    inputs = inputs.sigmoid()\n",
    "    inputs = inputs.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.sum() / num_boxes\n",
    "\n",
    "\n",
    "def sigmoid_focal_coef(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
    "    N, M = len(inputs), len(targets)\n",
    "    inputs = inputs.flatten(1).unsqueeze(1).expand(-1, M, -1)\n",
    "    targets = targets.flatten(1).unsqueeze(0).expand(N, -1, -1)\n",
    "\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(\n",
    "        inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    coef = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        coef = alpha_t * coef\n",
    "\n",
    "    return coef.mean(2)\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(\n",
    "        inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    return loss.mean(1).sum() / num_boxes\n",
    "\n",
    "\n",
    "\n",
    "class HungarianMatcherIFC(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cost_class: float = 1,\n",
    "        cost_dice: float = 1,\n",
    "        num_classes: int = 80,\n",
    "    ):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_mask: This is the relative weight of the sigmoid_focal error of the masks in the matching cost\n",
    "            cost_dice: This is the relative weight of the dice loss of the masks in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_dice = cost_dice\n",
    "        assert cost_class != 0 or cost_dice != 0, \"all costs cant be 0\"\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_cum_classes = [0] + \\\n",
    "            np.cumsum(np.array(num_classes) + 1).tolist()\n",
    "        self.n_future = 4\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"pred_logits\"].softmax(-1)\n",
    "        out_mask = outputs[\"pred_masks\"]\n",
    "        B, Q, T, s_h, s_w = out_mask.shape\n",
    "        t_h, t_w = targets[0][\"match_masks\"].shape[-2:]\n",
    "\n",
    "        if (s_h, s_w) != (t_h, t_w):\n",
    "            out_mask = out_mask.reshape(B, Q*T, s_h, s_w)\n",
    "            out_mask = torch.nn.F.interpolate(out_mask, size=(\n",
    "                t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "            out_mask = out_mask.view(B, Q, T, t_h, t_w)\n",
    "\n",
    "        indices = []\n",
    "        for b_i in range(B):\n",
    "            b_tgt_ids = targets[b_i][\"labels\"]\n",
    "            b_out_prob = out_prob[b_i]\n",
    "\n",
    "            cost_class = b_out_prob[:, b_tgt_ids]\n",
    "\n",
    "            b_tgt_mask = targets[b_i][\"match_masks\"]\n",
    "            b_out_mask = out_mask[b_i]\n",
    "\n",
    "            # Compute the dice coefficient cost between masks\n",
    "            # The 1 is a constant that doesn't change the matching as cost_class, thus omitted.\n",
    "            \n",
    "            cost_dice = dice_coef(\n",
    "                b_out_mask, b_tgt_mask\n",
    "            ).to(cost_class)\n",
    "\n",
    "            print(f\"{cost_dice.shape = } {cost_class.shape = } {b_tgt_ids.shape = } {b_out_prob.shape = } {b_tgt_mask.shape = } {b_out_mask.shape = }\")\n",
    "            # Final cost matrix\n",
    "            C = self.cost_dice * cost_dice + self.cost_class * cost_class\n",
    "\n",
    "            indices.append(linear_sum_assignment(C.cpu(), maximize=True))\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for IFC.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth masks and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and mask)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses, num_frames):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        self.num_frames = num_frames\n",
    "        empty_weight = torch.ones(num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_masks, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_masks]\n",
    "        \"\"\"\n",
    "        src_logits = outputs['pred_logits']\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J]\n",
    "                                     for t, (_, J) in zip(targets, indices)])\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(\n",
    "            1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - \\\n",
    "                accuracy(src_logits[idx], target_classes_o)[0]\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_masks):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor(\n",
    "            [len(v[\"labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) !=\n",
    "                     pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_masks(self, outputs, targets, indices, num_masks):\n",
    "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
    "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_masks, h, w]\n",
    "        \"\"\"\n",
    "        assert \"pred_masks\" in outputs\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_masks = outputs[\"pred_masks\"][idx]\n",
    "\n",
    "        target_masks = torch.cat(\n",
    "            [t['masks'][i] for t, (_, i) in zip(targets, indices)]).to(src_masks)\n",
    "\n",
    "        n, t = src_masks.shape[:2]\n",
    "        t_h, t_w = target_masks.shape[-2:]\n",
    "\n",
    "        src_masks = F.interpolate(src_masks, size=(\n",
    "            t_h, t_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        src_masks = src_masks.flatten(1)\n",
    "        target_masks = target_masks.flatten(1)\n",
    "\n",
    "        losses = {\n",
    "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_masks),\n",
    "            \"loss_dice\": dice_loss(src_masks, target_masks, num_masks),\n",
    "        }\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i)\n",
    "                              for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i)\n",
    "                              for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_masks, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'masks': self.loss_masks\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_masks, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        outputs_without_aux = {k: v for k,\n",
    "                               v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target masks accross all nodes, for normalization purposes\n",
    "        num_masks = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_masks = torch.as_tensor(\n",
    "            [num_masks], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "        # if is_dist_avail_and_initialized():\n",
    "        #     torch.distributed.all_reduce(num_masks)\n",
    "        num_masks = torch.clamp(num_masks / 1, min=1).item()\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(\n",
    "                loss, outputs_without_aux, targets, indices, num_masks))\n",
    "\n",
    "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
    "        if 'aux_outputs' in outputs:\n",
    "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
    "                indices = self.matcher(aux_outputs, targets)\n",
    "                for loss in self.losses:\n",
    "                    kwargs = {}\n",
    "                    if loss == 'labels':\n",
    "                        # Logging is enabled only for the last layer\n",
    "                        kwargs = {'log': False}\n",
    "                    l_dict = self.get_loss(\n",
    "                        loss, aux_outputs, targets, indices, num_masks, **kwargs)\n",
    "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
    "                    losses.update(l_dict)\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\" \n",
    "# 1. Get Code and shapes of in-/output  -- \n",
    "# 2. Get Matcher for the masks as well as postprocessing & loss function\n",
    "# 3. Test based on real GTs \n",
    "# \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
    "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout,padding=1,kernel_size=5, activation1= None,activation2=None):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin , kernel_size=kernel_size, padding=padding, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin , nout, kernel_size=1)\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation1(out)\n",
    "        out = self.pointwise(out)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation2(out)\n",
    "        return out\n",
    "\n",
    "class MaskHeadSmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple convolutional head, using group norm.\n",
    "    Upsampling is done using a FPN approach\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fpn_dims, output_dict=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # inter_dims = [dim, context_dim // 2, context_dim // 4,\n",
    "        #               context_dim // 8, context_dim // 16, context_dim // 64, context_dim // 128]\n",
    "        self.n_future = 5 \n",
    "        gn = 8\n",
    "         \n",
    "        self.lay1 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn1 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay2 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn2 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay3 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn3 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay4 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn4 = torch.nn.GroupNorm(gn, dim)\n",
    "        self.lay5 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        self.gn5 = torch.nn.GroupNorm(gn, dim)\n",
    "        \n",
    "        # self.lay6 = torch.nn.Conv2d(dim, dim, 3, padding=1)\n",
    "        # self.gn6 = torch.nn.GroupNorm(gn, dim)\n",
    "\n",
    "        self.depth_sep_conv2d =  depthwise_separable_conv(dim,dim,kernel_size=5,padding=2, activation1= F.relu,activation2= F.relu)\n",
    "\n",
    "        # half_dim = dim/2     \n",
    "        # self.out_lay_1 = torch.nn.Conv2d(\n",
    "        #     dim, half_dim, 3, padding=1)\n",
    "        # self.out_lay_2 = torch.nn.Conv2d(\n",
    "        #     half_dim, 1, 3, padding=1)  # <- This would be differen\n",
    "        \n",
    "        self.convert_to_weight = MLP(dim, dim, dim, 3)\n",
    "        # if output_dict is not None:\n",
    "        #     self.future_pred_layers = build_output_convs(\n",
    "        #         inter_dims[4], output_dict)\n",
    "        \"\"\" \n",
    "        outheads_\n",
    "            - motion_segmentation: 1x5x200x200   - BxFx1xHxW\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        self.adapter1 = torch.nn.Conv2d(fpn_dims[0], dim, 1)\n",
    "        self.adapter2 = torch.nn.Conv2d(fpn_dims[1], dim, 1)\n",
    "        self.adapter3 = torch.nn.Conv2d(fpn_dims[2], dim, 1)\n",
    "        self.adapter4 = torch.nn.Conv2d(fpn_dims[3], dim, 1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, src, seg_memory, fpns, hs ):\n",
    "        x = src + seg_memory\n",
    "        x = self.lay1(x)\n",
    "        x = self.gn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter1(fpns[0])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay2(x)\n",
    "        x = self.gn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter2(fpns[1])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay3(x)\n",
    "        x = self.gn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter3(fpns[2])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        #print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "        x = self.lay4(x)\n",
    "        x = self.gn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        cur_fpn = self.adapter4(fpns[3])\n",
    "        x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "        x = self.lay5(x)\n",
    "        x = self.gn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        T = self.n_future\n",
    "\n",
    "        x = x.unsqueeze(1).repeat(1,T,1,1,1)\n",
    "        B, BT, C, H, W = x.shape\n",
    "        L, B, N, C = hs.shape\n",
    "\n",
    "        x = self.depth_sep_conv2d(x.view(B*BT, C , H,W)).view(B,BT,C,H,W)\n",
    "\n",
    "\n",
    "        w = self.convert_to_weight(hs).permute(1,0,2,3)\n",
    "        print(w.shape)\n",
    "        w = w.unsqueeze(1).repeat(1,T,1,1,1)\n",
    "        print(w.shape)\n",
    "        \n",
    "        \n",
    "        mask_logits = F.conv2d(x.view(1, BT*C, H, W), w.reshape(B*T*L*N, C, 1, 1), groups=BT)\n",
    "        mask_logits = mask_logits.view(B, T, L, N, H, W).permute(2, 0, 3, 1, 4, 5)\n",
    "        return mask_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fpn3.shape = torch.Size([2, 256, 50, 50]), torch.Size([1, 256, 25, 25])\n",
      "interpolation, torch.Size([2, 256, 50, 50])\n",
      "after adapter1 torch.Size([2, 1280, 50, 50])\n",
      " adapter2 torch.Size([2, 1280, 100, 100]) torch.Size([2, 1280, 50, 50])\n",
      "2, 5 5\n",
      "after reshape torch.Size([2, 5, 256, 100, 100])\n",
      "HS input torch.Size([6, 2, 300, 256])\n",
      "after weight torch.Size([2, 6, 300, 1280])\n",
      "after reshape torch.Size([2, 5, 6, 300, 256])\n",
      "torch.Size([2, 5, 256, 100, 100])\n",
      "mask logits torch.Size([1, 18000, 100, 100])\n",
      "mask logits torch.Size([6, 2, 300, 5, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
    "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, padding=1, kernel_size=5, activation1=None, activation2=None):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            nin, nin, kernel_size=kernel_size, padding=padding, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "        self.activation1 = activation1\n",
    "        self.activation2 = activation2\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation1(out)\n",
    "        out = self.pointwise(out)\n",
    "        if self.activation1 is not None:\n",
    "            out = self.activation2(out)\n",
    "        return out\n",
    "    \n",
    "dim = 256\n",
    "gn=8\n",
    "num_queries = 300\n",
    "hidden_dim = dim \n",
    "T = 5 \n",
    "hs = torch.rand([6, 2, num_queries, hidden_dim])\n",
    "fpn3 = torch.rand((2, 256, 50, 50))\n",
    "fpn4 = torch.rand((2, 512, 100, 100))\n",
    "\n",
    "\n",
    "lay4 = torch.nn.Conv2d(dim, dim*T, 3, padding=1)\n",
    "gn4 = torch.nn.GroupNorm(gn, dim*T)\n",
    "lay5 = torch.nn.Conv2d(dim*2, dim*T, 3, padding=1)\n",
    "gn5 = torch.nn.GroupNorm(gn, dim*T)\n",
    "adapter3 = torch.nn.Conv2d(256, dim, 1)\n",
    "adapter4 = torch.nn.Conv2d(dim*2, dim*T, 1)\n",
    "convert_to_weight = MLP(dim, dim, dim*T, 2)\n",
    "depth_sep_conv2d = depthwise_separable_conv(\n",
    "    dim, dim, kernel_size=5, padding=2, activation1=F.relu, activation2=F.relu)\n",
    "\n",
    "\n",
    "a = nn.Sequential(\n",
    "    nn.Conv3d(dim, dim, kernel_size=[1, 1, 1], bias=False),\n",
    "    nn.BatchNorm3d(\n",
    "        num_features=dim, eps=1e-5, momentum=0.1\n",
    "    ),\n",
    "    nn.ReLU(inplace=True),\n",
    ")\n",
    "\n",
    "# Depthwise (channel-separated) 3x3x3x1 conv\n",
    "# Depthwise (channel-separated) 1x3x3x1 spatial conv\n",
    "b1 = nn.Conv3d(\n",
    "    dim,\n",
    "    dim,\n",
    "    kernel_size=[1, 3, 3],\n",
    "    stride=[1, 1, 1],\n",
    "    padding=[0, 1, 1],\n",
    "    bias=False,\n",
    ")\n",
    "# Depthwise (channel-separated) 3x1x1x1 temporal conv\n",
    "b2 = nn.Conv3d(\n",
    "    dim,\n",
    "    dim,\n",
    "    kernel_size=[3, 1, 1],\n",
    "    stride=[1, 1, 1],\n",
    "    padding=[1, 0, 0],\n",
    "    bias=False,\n",
    ")\n",
    "\n",
    "\n",
    "x = torch.rand((1, 256, 25, 25))\n",
    "print(f\"input {fpn3.shape = }, {x.shape}\")\n",
    "cur_fpn = adapter3(fpn3)\n",
    "x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "#print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "print(f\"interpolation, {x.shape}\")\n",
    "x = lay4(x)\n",
    "x = gn4(x)\n",
    "x = F.relu(x)\n",
    "\n",
    "print(f\"after adapter1 {x.shape }\")\n",
    "cur_fpn = adapter4(fpn4)\n",
    "print(f\" adapter2 {cur_fpn.shape } {x.shape}\")\n",
    "x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "\n",
    "T = 5\n",
    "H, W = x.shape[-2:]\n",
    "B = 2\n",
    "# x.unsqueeze(1).reshape(1, T, -1, H, W)\n",
    "x = x.unsqueeze(1).reshape(B, -1, T, H, W)\n",
    "x = b1(x)\n",
    "x = b2(x)\n",
    "x = F.relu(x)\n",
    "x = a(x).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "B, BT, C, H, W = x.shape\n",
    "L, B, N, C = hs.shape\n",
    "print(f\"{B}, {BT} {T}\")\n",
    "# x = depth_sep_conv2d(x.view(B*BT, C, H, W)).view(B, BT, C, H, W)\n",
    "\n",
    "print(f\"after reshape {x.shape }\")\n",
    "print(f\"HS input {hs.shape }\")\n",
    "w = convert_to_weight(hs).permute(1, 0, 2, 3)\n",
    "print(f\"after weight {w.shape }\")\n",
    "#torch.Size([1, 6, 100, 256])\n",
    "#torch.Size([1, 5, 6, 100, 256])\n",
    "w = w.unsqueeze(1).reshape(B, T, L, N, -1)\n",
    "print(f\"after reshape {w.shape }\")\n",
    "print(x.shape)\n",
    "x = x.reshape(1, B*BT*C, H, W)\n",
    "mask_logits = F.conv2d(x,\n",
    "                       w.reshape(B*T*L*N, C, 1, 1), groups=T*B)\n",
    "print(f\"mask logits {mask_logits.shape }\")\n",
    "mask_logits = mask_logits.view(\n",
    "    B, T, L, N, H, W).permute(2, 0, 3, 1, 4, 5)\n",
    "print(f\"mask logits {mask_logits.shape }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fpn3.shape = torch.Size([1, 256, 50, 50]), torch.Size([1, 256, 25, 25])\n",
      "interpolation, torch.Size([1, 256, 50, 50])\n",
      "after adapter1 torch.Size([1, 1280, 50, 50])\n",
      " adapter2 torch.Size([1, 1280, 100, 100]) torch.Size([1, 1280, 50, 50])\n",
      "after reshape torch.Size([1, 5, 256, 100, 100])\n",
      "HS input torch.Size([6, 1, 500, 256])\n",
      "after weight torch.Size([1, 6, 500, 1280])\n",
      "after reshape torch.Size([1, 5, 6, 500, 256])\n",
      "torch.Size([1, 5, 256, 100, 100])\n",
      "mask logits torch.Size([1, 5, 6, 500, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 256, 25, 25))\n",
    "print(f\"input {fpn3.shape = }, {x.shape}\")\n",
    "cur_fpn = adapter3(fpn3)\n",
    "x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "#print(f\"Interpolutaion with expan: {x.shape = }\")\n",
    "print(f\"interpolation, {x.shape}\")\n",
    "x = lay4(x)\n",
    "x = gn4(x)\n",
    "x = F.relu(x)\n",
    "\n",
    "print(f\"after adapter1 {x.shape }\")\n",
    "cur_fpn = adapter4(fpn4)\n",
    "print(f\" adapter2 {cur_fpn.shape } {x.shape}\")\n",
    "x = cur_fpn + F.interpolate(x, size=cur_fpn.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "\n",
    "T = 5\n",
    "H, W = x.shape[-2:]\n",
    "# x.unsqueeze(1).reshape(1, T, -1, H, W)\n",
    "x = x.unsqueeze(1).reshape(1, -1, T, H, W)\n",
    "x= b1(x)\n",
    "x=b2(x)\n",
    "x = F.relu(x)\n",
    "x = a(x).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "B, BT, C, H, W = x.shape\n",
    "L, B, N, C = hs.shape\n",
    "\n",
    "# x = depth_sep_conv2d(x.view(B*BT, C, H, W)).view(B, BT, C, H, W)\n",
    "\n",
    "print(f\"after reshape {x.shape }\")\n",
    "print(f\"HS input {hs.shape }\")\n",
    "w = convert_to_weight(hs).permute(1, 0, 2, 3)\n",
    "print(f\"after weight {w.shape }\")\n",
    "#torch.Size([1, 6, 100, 256])\n",
    "#torch.Size([1, 5, 6, 100, 256])\n",
    "w = w.unsqueeze(1).reshape(1, T, L, N, -1)\n",
    "print(f\"after reshape {w.shape }\")\n",
    "print(x.shape)\n",
    "x = x.reshape(1, BT*C, H, W)\n",
    "mask_logits = F.conv2d(x,\n",
    "                       w.reshape(B*T*L*N, C, 1, 1), groups=BT)\n",
    "print(f\"mask logits {w.shape }\")\n",
    "mask_logits = mask_logits.view(\n",
    "    B, T, L, N, H, W).permute(2, 0, 3, 1, 4, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 256, 100, 100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reshape = torch.rand((1, 1280, 100, 100))\n",
    "test_reshape= test_reshape.unsqueeze(1)\n",
    "test_reshape.reshape(1,T,-1,100,100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reshape = torch.rand((1, 1280, 100, 100))\n",
    "test_reshape.shape[-2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 100, 51])\n",
      "torch.Size([1, 6, 100, 256])\n",
      "torch.Size([1, 5, 6, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "n_future = 5\n",
    "hidden_dim = 256\n",
    "nheads = 8\n",
    "num_queries = 100\n",
    "num_classes = 50 \n",
    "\n",
    "gt_instance = torch.randint(low=0,high=2, size=(1, n_future, 200, 200)).to(torch.float32)\n",
    "seg_memory = torch.rand((1, hidden_dim, 13, 13))\n",
    "seg_mask = torch.randint(low=0, high=1, size=(1, 13, 13))\n",
    "hs = torch.rand([6, 1, num_queries, hidden_dim]) # N x B X Q x H <. N layers , B batchsize, query dim , hidden \n",
    "init_reference = torch.rand([1, num_queries, 2])\n",
    "srcs = torch.rand([1, hidden_dim, 13, 13])\n",
    "\n",
    "class_mlp = MLP(hidden_dim, hidden_dim, output_dim=num_classes + 1, num_layers=2)\n",
    "\n",
    "features = [ # with input projection\n",
    "    torch.rand((1, 256, 100, 100)), #64\n",
    "    torch.rand((1, 256, 50, 50)), #128\n",
    "    torch.rand((1, 256, 25, 25)), # 256\n",
    "    torch.rand((1, 256, 13, 13)), # 512 \n",
    "]\n",
    "input_projections = [(features[-1]),\n",
    "                     (features[-2]), (features[-3]), features[-4]]\n",
    "#\n",
    "fpn_dims = [256, 256, 256, 256]\n",
    "\n",
    "\n",
    "def _set_aux_loss( outputs_class, outputs_masks):\n",
    "    # this is a workaround to make torchscript happy, as torchscript\n",
    "    # doesn't support dictionary with non-homogeneous values, such\n",
    "    # as a dict having both a Tensor and a list.\n",
    "    return [{'pred_logits': a, 'pred_masks': b}\n",
    "            for a, b in zip(outputs_class[:-1], outputs_masks[:-1])]\n",
    "aux_loss = True \n",
    "class_logits_list = []\n",
    "for i in range(n_future):\n",
    "    class_logits_list.append( class_mlp(hs[-1]))\n",
    "\n",
    "outputs_class = torch.stack(class_logits_list)\n",
    "print(outputs_class.shape)\n",
    "mask_head = MaskHeadSmallConv(hidden_dim,fpn_dims)\n",
    "\n",
    "outputs_masks = mask_head(\n",
    "        srcs, seg_memory, input_projections,hs)\n",
    "\n",
    "\n",
    "out = {'pred_logits': outputs_class[-1]}\n",
    "out.update({'pred_masks': outputs_masks[-1]})\n",
    "\n",
    "if aux_loss:\n",
    "    out['aux_outputs'] = _set_aux_loss(outputs_class, outputs_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_frames = 5\n",
    "dice_weight=3.0\n",
    "mask_weight=3.0\n",
    "no_object_weight = 0.1 \n",
    "deep_supervision = True\n",
    "dec_layers = 3\n",
    "\n",
    "\n",
    "matcher = HungarianMatcherIFC(\n",
    "    cost_class=1,\n",
    "    cost_dice=dice_weight,\n",
    "    num_classes=num_classes,\n",
    "    )\n",
    "weight_dict = {\"loss_ce\": 1, \"loss_mask\": mask_weight,\n",
    "                \"loss_dice\": dice_weight}\n",
    "if deep_supervision:\n",
    "    aux_weight_dict = {}\n",
    "    for i in range(dec_layers - 1):\n",
    "        aux_weight_dict.update(\n",
    "            {k + f\"_{i}\": v for k, v in weight_dict.items()})\n",
    "    weight_dict.update(aux_weight_dict)\n",
    "losses = [\"labels\", \"masks\", \"cardinality\"]\n",
    "criterion = SetCriterion(\n",
    "    num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=no_object_weight, losses=losses,\n",
    "    num_frames=num_frames\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects.mmdet3d_plugin\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from projects.mmdet3d_plugin.datasets.utils.warper import FeatureWarper\n",
    "import os\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from mmcv import Config\n",
    "\n",
    "\n",
    "def import_modules_load_config(cfg_file=\"beverse_tiny.py\", samples_per_gpu=1):\n",
    "    cfg_path = r\"/home/niklas/ETM_BEV/BEVerse/projects/configs\"\n",
    "    cfg_path = os.path.join(cfg_path, cfg_file)\n",
    "\n",
    "    cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "    # if args.cfg_options is not None:\n",
    "    #     cfg.merge_from_dict(args.cfg_options)\n",
    "    # import modules from string list.\n",
    "    if cfg.get(\"custom_imports\", None):\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "\n",
    "        import_modules_from_strings(**cfg[\"custom_imports\"])\n",
    "\n",
    "    # import modules from plguin/xx, registry will be updated\n",
    "    if hasattr(cfg, \"plugin\"):\n",
    "        if cfg.plugin:\n",
    "            import importlib\n",
    "\n",
    "            if hasattr(cfg, \"plugin_dir\"):\n",
    "                plugin_dir = cfg.plugin_dir\n",
    "                _module_dir = os.path.dirname(plugin_dir)\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "            else:\n",
    "                # import dir is the dirpath for the config file\n",
    "                _module_dir = cfg_path\n",
    "                _module_dir = _module_dir.split(\"/\")\n",
    "                _module_path = _module_dir[0]\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + \".\" + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = cfg.data.test.pop(\"samples_per_gpu\", 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                cfg.data.test.pipeline)\n",
    "    elif isinstance(cfg.data.test, list):\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop(\"samples_per_gpu\", 1) for ds_cfg in cfg.data.test]\n",
    "        )\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "cfg = import_modules_load_config(\n",
    "    cfg_file=r\"beverse_tiny_org.py\")\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=2,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False)\n",
    "\n",
    "\n",
    "grid_conf = {\n",
    "    \"xbound\": [-50.0, 50.0, 0.5],\n",
    "    \"ybound\": [-50.0, 50.0, 0.5],\n",
    "    \"zbound\": [-10.0, 10.0, 20.0],\n",
    "    \"dbound\": [1.0, 60.0, 1.0],\n",
    "}\n",
    "\n",
    "warper = FeatureWarper(grid_conf=grid_conf)\n",
    "\n",
    "\n",
    "class pseud_class:\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        self.receptive_field = 3\n",
    "        self.warper = FeatureWarper(grid_conf=grid_conf)\n",
    "        \n",
    "    def prepare_targets(self, batch,bev_size = (200,200), mask_stride=2,match_stride=2):\n",
    "        segmentation_labels = batch[\"motion_segmentation\"][0]\n",
    "        gt_instance = batch[\"motion_instance\"][0]\n",
    "        future_egomotion = batch[\"future_egomotions\"][0]\n",
    "        batch_size = len(segmentation_labels)\n",
    "        labels = {}\n",
    "\n",
    "        bev_transform = batch.get(\"aug_transform\", None)\n",
    "        labels[\"img_is_valid\"] = batch.get(\"img_is_valid\", None)\n",
    "\n",
    "        if bev_transform is not None:\n",
    "            bev_transform = bev_transform.float()\n",
    "\n",
    "        # Warp instance labels to present's reference frame\n",
    "        gt_instance = (\n",
    "            self.warper.cumulative_warp_features_reverse(\n",
    "                gt_instance.float().unsqueeze(2),\n",
    "                future_egomotion[:, (self.receptive_field - 1) :],\n",
    "                mode=\"nearest\",\n",
    "                bev_transform=bev_transform,\n",
    "            )\n",
    "            .long()\n",
    "            .contiguous()[:, :, 0]\n",
    "        )\n",
    "        # better solution by abdur but unsure how to make it work with the rest of the code specifcally maxID since it can be diffferent for batches\n",
    "        # temp = torch.arange(MaxID).unsqueeze(0).repeat(B, 1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        # gt_masks_ifc_dim  = (temp== Target.unsqueeze(1)).float() \n",
    "        target_list = []\n",
    "        for b in range(batch_size):\n",
    "            gt_list = []\n",
    "            ids = len(gt_instance[b].unique())\n",
    "            for _id in range(ids):\n",
    "                test_bool = torch.where(gt_instance[b] == _id,1.,0.)\n",
    "                gt_list.append(test_bool)\n",
    "\n",
    "            segmentation_labels = torch.stack(gt_list,dim=0)\n",
    "            \n",
    "            #segmentation_labels = torch.stack(gt_batch_instances_list,dim=0)\n",
    "            o_h, o_w = bev_size\n",
    "            l_h, l_w = math.ceil(o_h/mask_stride), math.ceil(o_w/mask_stride)\n",
    "            m_h, m_w = math.ceil(o_h/match_stride), math.ceil(o_w/match_stride)\n",
    "\n",
    "            gt_masks_for_loss  = F.interpolate(segmentation_labels, size=(l_h, l_w), mode=\"bilinear\", align_corners=False)\n",
    "            gt_masks_for_match = F.interpolate(segmentation_labels, size=(m_h, m_w), mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            ids = gt_instance[b].unique() # labels only continous for clip - this is much more of an tracking id as every class is a vehicle anyways # TODO make work with other types of superclasses other then vehicle\n",
    "            target_list.append({\"labels\": ids,\"masks\": gt_masks_for_loss, \"match_masks\": gt_masks_for_match, \"gt_motion_instance\":gt_instance[b] })\n",
    "        return target_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=pseud_class()\n",
    "target_list = p.prepare_targets(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5, 100, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_list[0][\"match_masks\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_dice.shape = torch.Size([100, 15]) cost_class.shape = torch.Size([100, 15]) b_tgt_ids.shape = torch.Size([15]) b_out_prob.shape = torch.Size([100, 51]) b_tgt_mask.shape = torch.Size([15, 5, 100, 100]) b_out_mask.shape = torch.Size([100, 5, 100, 100])\n",
      "cost_dice.shape = torch.Size([100, 15]) cost_class.shape = torch.Size([100, 15]) b_tgt_ids.shape = torch.Size([15]) b_out_prob.shape = torch.Size([100, 51]) b_tgt_mask.shape = torch.Size([15, 5, 100, 100]) b_out_mask.shape = torch.Size([100, 5, 100, 100])\n",
      "cost_dice.shape = torch.Size([100, 15]) cost_class.shape = torch.Size([100, 15]) b_tgt_ids.shape = torch.Size([15]) b_out_prob.shape = torch.Size([100, 51]) b_tgt_mask.shape = torch.Size([15, 5, 100, 100]) b_out_mask.shape = torch.Size([100, 5, 100, 100])\n",
      "cost_dice.shape = torch.Size([100, 15]) cost_class.shape = torch.Size([100, 15]) b_tgt_ids.shape = torch.Size([15]) b_out_prob.shape = torch.Size([100, 51]) b_tgt_mask.shape = torch.Size([15, 5, 100, 100]) b_out_mask.shape = torch.Size([100, 5, 100, 100])\n",
      "cost_dice.shape = torch.Size([100, 15]) cost_class.shape = torch.Size([100, 15]) b_tgt_ids.shape = torch.Size([15]) b_out_prob.shape = torch.Size([100, 51]) b_tgt_mask.shape = torch.Size([15, 5, 100, 100]) b_out_mask.shape = torch.Size([100, 5, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "loss_dict = criterion(out, target_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target class torch.Size([16, 100])\n",
    "src_logits transposed 1 2 torch.Size([16, 100, 41])\n",
    "empty weight torch.Size([41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 100])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_masks_stacked = torch.stack(out[\"pred_masks\"]).transpose_(1,0)\n",
    "# list of BxQxHxW\n",
    "pred_masks_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_instances[0][\"match_masks\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prob = torch.stack(out[\"pred_logits\"]).transpose(1, 0).softmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_prob \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39;49m\u001b[39mpred_logits\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49msoftmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "out_prob = out[\"pred_logits\"].softmax(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 100, 100])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_mask = torch.stack(out[\"pred_masks\"]).transpose(1, 0)\n",
    "out_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49a6cb26e152f15aca94d1d3fa9630fb57fb8fd83a336982cd2ebf9e9635e69c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
